---
title: "Analysis"
author: "Soo Wan Kim"
date: "April 28, 2017"
output:
  github_document
---

# The Role of Color in Proportional Judgment in Bar Charts

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(knitr)
library(ggmap)
library(ggthemes)
library(broom)

setwd("~/GitHub/dataviz/submissions/kim_soowan/Assignment 2")

theme_set(theme_minimal())

##################
# Correct answers #
##################

proportions <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(proportions) <- c("question_id", "prop")

files <- list.files("Experimental Design/Proportions Data")
path_root <- "Experimental Design/Proportions Data/"

import_select <- function(file) {
  path <- paste0(path_root, file)
  data <- read_csv(path)
  data <- dplyr::filter(data, !is.na(dot))
  data <- data[,2]
  
  question_id <- as.numeric(unlist(regmatches(path, gregexpr("[0-9]", path))))
  prop <- min(data)/max(data) * 100
  
  prop_data <- as.data.frame(t(as.data.frame(c(question_id, prop))))
  rownames(prop_data) <- NULL
  colnames(prop_data) <- c("question_id", "prop")
  
  return(prop_data)
}

for (i in seq_along(files)) {
  prop <- import_select(files[i])
  proportions <- rbind(proportions, prop)
}

rm(prop)

###################
# Tidy Test Data  #
###################

#Treatment 1 - Black and White

treatment1 <- read_csv("Test Data/Treatment 1 - Black and White.csv") %>%
  filter(Status == "IP Address", Finished == "TRUE")

treatment1_filt <- treatment1 %>%
  select(-RecipientLastName, -RecipientFirstName, -RecipientEmail, 
         -ExternalReference, -ResponseId, -Status, -Finished) %>%
  transform(Q1 = toupper(Q1)) %>%
  filter(Q1 %in% c("A", "C", "A20")) %>%
  transform(Q2 = gsub("%", "", Q2)) %>%
  transform(Q2 = gsub("4-Jan", "25", Q2)) %>%
  transform(Q2 = ifelse(grepl("[0-9]", Q2), Q2, NA)) %>%
  transform(Q3 = gsub("%", "", Q3)) %>%
  transform(Q3 = ifelse(grepl("[0-9]", Q3), Q3, NA)) %>%
  transform(Q4 = gsub("%", "", Q4)) %>%
  transform(Q4 = ifelse(grepl("[0-9]", Q4), Q4, NA)) %>%
  transform(Q4 = gsub("2-Jan", "50", Q4)) %>%
  transform(Q5 = gsub("%", "", Q5)) %>%
  transform(Q5 = ifelse(grepl("[0-9]", Q5), Q5, NA)) %>%
  transform(Q6 = gsub("%", "", Q6)) %>%
  transform(Q6 = ifelse(grepl("[0-9]", Q6), Q6, NA)) %>%
  transform(Q6 = gsub("2-Jan", "50", Q6)) %>%
  transform(Q2 = as.numeric(Q2)) %>%
  transform(Q2 = ifelse(Q2 > 100, 1/(Q2/100) * 100, Q2)) %>%
  transform(Q3 = as.numeric(Q3)) %>%
  transform(Q3 = ifelse(Q3 > 100, 1/(Q3/100) * 100, Q3)) %>%
  transform(Q4 = as.numeric(Q4)) %>%
  transform(Q4 = ifelse(Q4 > 100, 1/(Q4/100) * 100, Q4)) %>%
  transform(Q5 = as.numeric(Q5)) %>%
  transform(Q5 = ifelse(Q5 > 100, 1/(Q5/100) * 100, Q5)) %>%
  transform(Q6 = as.numeric(Q6)) %>%
  transform(Q6 = ifelse(Q6 > 100, 1/(Q6/100) * 100, Q6)) %>%
  mutate(treatment = 1) %>%
  na.omit()

#Treatment 2 - All Colored

treatment2 <- read_csv("Test Data/Treatment 2 - All Color.csv") %>%
  filter(Status == "IP Address", Finished == "TRUE")

treatment2_filt <- treatment2 %>%
  select(-RecipientLastName, -RecipientFirstName, -RecipientEmail, 
         -ExternalReference, -ResponseId, -Status, -Finished) %>%
  transform(Q1 = toupper(Q1)) %>%
  filter(Q1 %in% c("A", "C", "1")) %>%
  transform(Q2 = gsub("%", "", Q2)) %>%
  transform(Q2 = gsub(" PERCENTAGE", "", Q2)) %>%
  transform(Q2 = ifelse(grepl("^[0-9]", Q2), Q2, NA)) %>%
  transform(Q3 = gsub("%", "", Q3)) %>%
  transform(Q3 = gsub(" PERCENTAGE", "", Q3)) %>%
  transform(Q3 = gsub("B. ", "", Q3)) %>%
  transform(Q3 = gsub("9580", "95", Q3)) %>%
  transform(Q3 = ifelse(grepl("[0-9]", Q3), Q3, NA)) %>%
  transform(Q4 = gsub("%", "", Q4)) %>%
  transform(Q4 = gsub(" PERCENTAGE", "", Q4)) %>%
  transform(Q4 = ifelse(grepl("^[0-9]", Q4), Q4, NA)) %>%
  transform(Q5 = gsub("%", "", Q5)) %>%
  transform(Q5 = gsub(" PERCENTAGE", "", Q5)) %>%
  transform(Q5 = gsub("appear equal", "100", Q5)) %>%
  transform(Q5 = ifelse(grepl("[0-9]", Q5), Q5, NA)) %>%
  transform(Q6 = gsub("%", "", Q6)) %>%
  transform(Q6 = gsub(" PERCENTAGE", "", Q6)) %>%
  transform(Q6 = ifelse(grepl("[0-9]", Q6), Q6, NA)) %>%
  transform(Q2 = as.numeric(Q2)) %>%
  transform(Q2 = ifelse(Q2 > 100, 1/(Q2/100) * 100, Q2)) %>%
  transform(Q3 = as.numeric(Q3)) %>%
  transform(Q3 = ifelse(Q3 > 100, 1/(Q3/100) * 100, Q3)) %>%
  transform(Q4 = as.numeric(Q4)) %>%
  transform(Q4 = ifelse(Q4 > 100, 1/(Q4/100) * 100, Q4)) %>%
  transform(Q5 = as.numeric(Q5)) %>%
  transform(Q5 = ifelse(Q5 > 100, 1/(Q5/100) * 100, Q5)) %>%
  transform(Q6 = as.numeric(Q6)) %>%
  transform(Q6 = ifelse(Q6 > 100, 1/(Q6/100) * 100, Q6)) %>%
  mutate(treatment = 2) %>%
  na.omit()

#Treatment 3 - Selectively Colored

treatment3 <- read_csv("Test Data/Treatment 3 - Selectively Colored.csv") %>%
  filter(Status == "IP Address", Finished == "TRUE")

treatment3_filt <- treatment3 %>%
  select(-RecipientLastName, -RecipientFirstName, -RecipientEmail, 
         -ExternalReference, -ResponseId, -Status, -Finished) %>%
  transform(Q1 = toupper(Q1)) %>%
  filter(Q1 %in% c("A", "C", "BC", "50", "20")) %>%
  transform(Q2 = gsub("%", "", Q2)) %>%
  transform(Q2 = gsub("4$", "40", Q2)) %>%
  transform(Q2 = ifelse(grepl("^[0-9]", Q2), Q2, NA)) %>%
  transform(Q3 = gsub("%", "", Q3)) %>%
  transform(Q3 = gsub("0.25", "25", Q3)) %>%
  transform(Q3 = gsub("7-Jan", "0.1428571", Q3)) %>%
  transform(Q3 = ifelse(grepl("^[0-9]", Q3), Q3, NA)) %>%
  transform(Q4 = gsub("%", "", Q4)) %>%
  transform(Q4 = gsub("0.5", "50", Q4)) %>%
  transform(Q4 = gsub("8597", "85", Q4)) %>%
  transform(Q4 = ifelse(grepl("^[0-9]", Q4), Q4, NA)) %>%
  transform(Q5 = gsub("%", "", Q5)) %>%
  transform(Q5 = ifelse(grepl("^[0-9]", Q5), Q5, NA)) %>%
  transform(Q6 = gsub("%", "", Q6)) %>%
  transform(Q6 = ifelse(nchar(Q6) > 2, NA, Q6)) %>%
  transform(Q6 = ifelse(grepl("[0-9]", Q6), Q6, NA)) %>%
  transform(Q2 = as.numeric(Q2)) %>%
  transform(Q2 = ifelse(Q2 > 100, 1/(Q2/100) * 100, Q2)) %>%
  transform(Q3 = as.numeric(Q3)) %>%
  transform(Q3 = ifelse(Q3 > 100, 1/(Q3/100) * 100, Q3)) %>%
  transform(Q4 = as.numeric(Q4)) %>%
  transform(Q4 = ifelse(Q4 > 100, 1/(Q4/100) * 100, Q4)) %>%
  transform(Q5 = as.numeric(Q5)) %>%
  transform(Q5 = ifelse(Q5 > 100, 1/(Q5/100) * 100, Q5)) %>%
  transform(Q6 = as.numeric(Q6)) %>%
  transform(Q6 = ifelse(Q6 > 100, 1/(Q6/100) * 100, Q6)) %>%
  mutate(treatment = 3) %>%
  na.omit()

#Combined

experimental_data <- rbind(treatment1_filt, treatment2_filt, treatment3_filt) %>%
  mutate(Q2err = proportions$prop[proportions$question_id == 2] - Q2) %>%
  mutate(Q3err = proportions$prop[proportions$question_id == 3] - Q3) %>%
  mutate(Q4err = proportions$prop[proportions$question_id == 4] - Q4) %>%
  mutate(Q5err = proportions$prop[proportions$question_id == 5] - Q5) %>%
  mutate(Q6err = proportions$prop[proportions$question_id == 6] - Q6) %>%
  mutate(combined_err = abs(Q2err) + abs(Q3err) + abs(Q4err) + abs(Q5err) + abs(Q6err)) %>%
  mutate(dodged_err = Q2err + Q4err) %>%
  mutate(stacked_side_err = Q3err + Q5err) %>%
  rename(Duration = Duration..in.seconds.) %>%
  transform(Duration = as.numeric(Duration)) %>%
  transform(RecordedDate = as.POSIXct(strptime(RecordedDate, "%m/%d/%Y %H:%M"))) %>%
  transform(LocationLatitude = as.numeric(LocationLatitude)) %>%
  transform(LocationLongitude = as.numeric(LocationLongitude)) %>%
  transform(treatment = as.factor(treatment))
```

## Introduction

This study uses experimental data drawn from the Amazon Mechanical Turk crowdsourcing platform to address the question: "How does color affect proportional judgment in bar charts?"

I tested the following hypotheses:

* H1: The color of bars has no effect on proportional judgment.
* H2: Color in bar charts serves primarily to attract and hold attention, which improves judgment of proportion by improving attentiveness. Thus, proportional judgment should be improved in colored bar charts relative to bar charts without color, but selective or indiscriminate use of color should not matter as much.
* H3: Selectively applying color to the relevant parts of the chart facilitates focus on relevant information and leads to better judgments of proportion, more so than if color were applied indiscriminately. 

Using means comparisons, I find significant support for the argument that selectively applying color helps focus attention to the relevant parts of a chart and thus faciliates proportional judgments in bar charts. I find weaker support for the argument that selective use of color aids prportional judgment more so than indiscriminate coloring. However, flaws in the experimental design preclude strong arguments in support of either finding.

## Data

As in Heer and Bostock's 2010 study, the experiment sample were recruited from Amazon Mechanical Turk (MTurk). Across all treatment groups, the task was to pick out the shorter of two bars or bar segments within a bar chart and estimate the percentage of the height of the taller bar that the height of the shorter bar made up. The bar charts were designed based on Cleveland and McGill (1984)'s proportional judgments experiment and Heer and Bostock (2010)'s replication of the same. There were two types: stacked or segmented bar charts with two bars each where the bars each had four segments representing different sub-groups, and non-stacked bar charts with four bars each where the bars were not segmented. 

There were three treatment groups: in treatment 1, participants viewed only black and white bar charts, in treament 2, participants looked at bar charts where all bars had a different color, and in treatment 3, participants looked at bar charts where only the relevant bars or segments were colored and the rest of the chart was black and white. Aside from the coloring, the plots were identical across treatments. The instructions and prompts were also the same. The plots and questions used for the study can be viewed in the "Experimental Design" folder.

```{r num_obs}
#subject data

#N
total_count1 <- nrow(treatment1)
total_count2 <- nrow(treatment2)
total_count3 <- nrow(treatment3)
filt_count1 <- nrow(treatment1_filt)
filt_count2 <- nrow(treatment2_filt)
filt_count3 <- nrow(treatment3_filt)
total_count_collected <- total_count1 + total_count2 + total_count3
total_count_filt <- nrow(experimental_data)

total_discarded <- total_count_collected - total_count_filt
perc_discarded <- round(total_discarded/total_count_collected*100,2)
```

The experiment was conducted via online survey using Qualtrics. The treatments were administered by means of three independent surveys, linked from three separate HITs on MTurk. Each treatment group included about 70 individuals for a total of `r total_count_collected` participants. However, `r perc_discarded`% of the data was unusable, leaving observations from `r total_count_filt` unique participants with around 50 individuals in each treatment group. Each survey included one screening question and five proprortional judgment questions, including two non-stacked bar charts and three stacked bar charts. Thus, excluding the screening question the final data set included `r total_count_filt` x 5 or `r total_count_filt*5` observations. 

The screening question asked the participant to pick out the bar representing the lowest value from among four bars. It was somewhat of a trick question, however, since two of the bars were marked from the rest, as if asking the participant to compare between the two bars only. Slightly more than half of respondents either answered correctly with the lowest bar out of the four or incorrectly with the lower bar out of the two marked bars. The responses with the latter answer to the screening question were not filtered out provided that the rest of the answers were appropriate, i.e. numeric responses indicating percentages. All responses where the participants provided letters representing the bars rather than percentages were discarded. 

Feedback from some initially rejected workers suggested that the wording of the questions was problematic. For example, a few workers expressed that they wrote down the estimaged percentage of the height of the taller bar relative to the shorter bar, mistakenly believing this to be the required response. Thus, I retained responses where percentages were larger than 100 and converted them to the appropriate form by taking the inverse and multiplying by 10,000. 

```{r n_table}
treatment <- c("1","2","3","All")
collected <- c(total_count1, total_count2, total_count3, total_count_collected)
used <- c(filt_count1, filt_count2, filt_count3, total_count_filt)

treatment <- as.data.frame(treatment)
collected <- as.data.frame(collected)
used <- as.data.frame(used)

n_table <- cbind(treatment, collected, used) 

n_table %>%
  kable(caption = "Number of observations per treatment group, collected and used",
        col.names = c("Treatment\nGroup", "Collected", "Used"),
        format = "html")
```

All three tasks were released on MTurk around 2 AM on the same day. However, due to discrepancies in the time periods where I checked, rejected and re-released submissions, each task covered somewhat different time periods between roughly 3 AM and 9 AM. In particular, more of task 2 was completed later in the morning relative to task 1 and task 3 was completed later than the other two tasks. This could have affected the results, particularly since workers from English-speaking countries are more likely to be awake later in the morning.

```{r task_timeline}
ggplot(experimental_data, aes(RecordedDate)) +
  geom_histogram() + 
  facet_wrap(~treatment) + 
  labs(title = "Time of experiment, by treatment group",
       subtitle = "N = 151",
       x = "Time Recorded (AM)",
       y = "Count") + 
  theme_bw()
```

Based on geolocations data from Qualtrics, it appears most of the workers reside in the US or India. There were somewhat more workers from the US and UK in treatment group 3 compared to the other treatment groups, as the above timelines would suggest.

```{r worker_locations}

#Where from

world <- map_data("world")
ggplot() + 
  geom_map(data=world, map=world,
                    aes(x=long, y=lat, map_id=region),
                    color="white", fill="#7f7f7f", size=0.05, alpha=1/4) + 
  geom_point(data=experimental_data, aes(LocationLongitude, LocationLatitude, 
                                         color = treatment, fill = treatment), size = 1, alpha = 1/2) + 
  theme_map() + 
  facet_wrap(~treatment, 2) + 
  labs(title = "Participant locations by treatment group")

us_map <- get_map(location = 'usa', source = "google", maptype = "roadmap",
                   crop = FALSE, zoom = 4)

ggmap(us_map) + 
  geom_point(data = experimental_data, 
             aes(x = LocationLongitude, y = LocationLatitude), color = "red", 
             shape = 16, alpha = 1/3) + 
  facet_wrap(~treatment) + 
  theme_map() + 
  labs(title = "Participant locations by treatment group (US only)")
```
```{r completion_time1}
avg_dur <- round(mean(experimental_data$Duration),2)
```

The average time to complete each HIT was `r avg_dur` seconds, or about 3 minutes.

```{r completion_time2}
completion_time_byq <- experimental_data %>%
  group_by(treatment) %>%
  summarize(dur_mean = round(mean(Duration),2))

dur_bottom_row <- as.data.frame(t(c("All", round(avg_dur,2))))
colnames(dur_bottom_row) <- c("treatment", "dur_mean")

dur_table <- rbind(completion_time_byq, dur_bottom_row)

dur_table %>%
  kable(caption = "Time taken to complete experiment",
        col.names = c("Treatment\nGroup", "Time (seconds)"),
        format = "html")
```

### Note on ethics

All initially rejected workers were approved and paid, considering that I did use their responses for analysis, even if only to analyze the rate of successful completion. All personally identifiable information such as IP Address and MTurk Worker ID were removed from the data. Geolocation data was included only to analyze the geographical spread of the participants.

## Results

The following table shows the mean of the absolute value of the error for all five questions by treatment group. Treatment group 3 or the group shown selectively colored bar charts performed the best, lending support to hypothesis 3. However, it appears the type of question may also have played an important role in the overall results. Participants performed much better in questions 2 and 3, the questions involving non-stacked bar charts, compared to the rest of the questions which involved stacked or segmented bar charts, in which performance was quite poor across the board. This suggests either that the segmenting induced confusion with the added difficulty of addressing the relative positioning on top of relative height, or simply induced confusion. It is possible that some workers did not properly understand the question and compared the heights of the whole bars rather than the segments. 

```{r avg_results_}
#Average results (absolute error)

err_means_byq <- experimental_data %>%
  group_by(treatment) %>%
  summarize(Q2err_mean = round(mean(abs(Q2err)),2), Q4err_mean = round(mean(abs(Q4err)),2),
            Q3err_mean = round(mean(abs(Q3err)),2), Q5err_mean = round(mean(abs(Q5err)),2),
            Q6err_mean = round(mean(abs(Q6err)),2), combinederr_mean = round(mean(abs(combined_err)),2)) %>%
  transform(treatment = as.character(treatment))

err_means_allq <- experimental_data %>%
  summarize(Q2err_mean = round(mean(abs(Q2err)),2), Q4err_mean = round(mean(abs(Q4err)),2),
            Q3err_mean = round(mean(abs(Q3err)),2), Q5err_mean = round(mean(abs(Q5err)),2),
            Q6err_mean = round(mean(abs(Q6err)),2), combinederr_mean = round(mean(abs(combined_err)),2))

all_groups <- data.frame(c("All"))
colnames(all_groups) <- c("treatment")
err_means_allq <- cbind(all_groups, err_means_allq)

err_table <- rbind(err_means_byq, err_means_allq)

err_table %>%
  kable(caption = "Mean absolute error across questions",
        col.names = c("Treatment\nGroup", "Q2", "Q4", "Q3", "Q5", "Q6", "All Questions"),
        format = "html")
```

The amount of time taken to complete the task does not appear to have had a signficant effect on performance, as the graph below demonstrates.

```{r duration_vs_error}
ggplot(experimental_data, (aes(combined_err, Duration))) + 
  geom_point(aes(color = treatment)) + 
  geom_smooth(aes(color = treatment)) + 
  labs(title = "Test completion time vs. mean absolute error (summed across all questions)",
       x = "Mean absolute error",
       y = "Completion time (seconds)",
       color = "Treatment\nGroup",
       caption = "N = 151")
``` 

To test the hypotheses further, I use visualizations and the Wilcoxon-Mann-Whitney rank sum test for comparing means without assuming a normal distribution. The two graphs below show the distributions of absolute error averaged across all questions for each of the treatment groups. The first graph uses all observations, while the second uses all observations collected before 4 AM. Controlling for time should indirectly control for worker location.

```{r err_viz}
ggplot(experimental_data, aes(combined_err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Absolute error distribution across all questions",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")

early_time <- experimental_data %>%
  filter(RecordedDate <= "2017-04-27 04:00:00")

ggplot(early_time, aes(combined_err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Absolute error distribution across all questions",
       subtitle = "Responses recorded before 4 AM",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 71")
```

The following chart shows the results from applying the Wilconox test on all combinations of treatment groups for all questions. The results indicate that there is not a significant difference in mean error between treatment groups 1 and 2 for all questions, suggesting that indiscrimately coloring all bars does not significantly aid proportional judgment relative to black and white charts for either stacked or non-stacked bar charts. However, in most cases we find a significant difference in means between groups 1 and 3. This suggests that selective coloring improves proportional judgment relative to no coloring. Also, when comparing differences in the means of absolute error for all questions combined, we see that there is a signficant difference between groups 2 and 3. This lends weaker support to the argument that selective coloring aids proportional judgment more so than indiscriminate coloring. 

```{r wilconox}
treatment1_filt <- experimental_data %>%
  filter(treatment == 1)

treatment2_filt <- experimental_data %>%
  filter(treatment == 2)

treatment3_filt <- experimental_data %>%
  filter(treatment == 3)

Q2_err1 <- treatment1_filt$Q2err
Q2_err2 <- treatment2_filt$Q2err
Q2_err3 <- treatment3_filt$Q2err
Q3_err1 <- treatment1_filt$Q3err
Q3_err2 <- treatment2_filt$Q3err
Q3_err3 <- treatment3_filt$Q3err
Q4_err1 <- treatment1_filt$Q4err
Q4_err2 <- treatment2_filt$Q4err
Q4_err3 <- treatment3_filt$Q4err
Q5_err1 <- treatment1_filt$Q5err
Q5_err2 <- treatment2_filt$Q5err
Q5_err3 <- treatment3_filt$Q5err
Q6_err1 <- treatment1_filt$Q6err
Q6_err2 <- treatment2_filt$Q6err
Q6_err3 <- treatment3_filt$Q6err
combined_err1 <- treatment1_filt$combined_err
combined_err2 <- treatment2_filt$combined_err
combined_err3 <- treatment3_filt$combined_err

question <- as.data.frame((c(rep("2", 3), rep("3", 3), 
                              rep("4", 3), rep("5", 3), rep("6", 3), rep("All (abs. error)", 3))))
colnames(question) <- c("Question")
comparison <- as.data.frame(c(rep(c("1 & 2", "1 & 3", "2 & 3"), 6)))
colnames(comparison) <- c("Groups Compared")

wilcox <- rbind(#Q2
  as.data.frame(tidy(wilcox.test(Q2_err1, Q2_err2))[,1:2]),
                as.data.frame(tidy(wilcox.test(Q2_err1, Q2_err3))[,1:2]),
                as.data.frame(tidy(wilcox.test(Q2_err2, Q2_err3))[,1:2]),
                #Q3
as.data.frame(tidy(wilcox.test(Q3_err1, Q3_err2))[,1:2]),
as.data.frame(tidy(wilcox.test(Q3_err1, Q3_err3))[,1:2]),
as.data.frame(tidy(wilcox.test(Q3_err2, Q3_err3))[,1:2]),
#Q4
as.data.frame(tidy(wilcox.test(Q4_err1, Q4_err2))[,1:2]),
as.data.frame(tidy(wilcox.test(Q4_err1, Q4_err3))[,1:2]),
as.data.frame(tidy(wilcox.test(Q4_err2, Q4_err3))[,1:2]),
#Q5
as.data.frame(tidy(wilcox.test(Q5_err1, Q5_err2))[,1:2]),
as.data.frame(tidy(wilcox.test(Q5_err1, Q5_err3))[,1:2]),
as.data.frame(tidy(wilcox.test(Q5_err2, Q5_err3))[,1:2]),
#Q6
as.data.frame(tidy(wilcox.test(Q6_err1, Q6_err2))[,1:2]),
as.data.frame(tidy(wilcox.test(Q6_err1, Q6_err3))[,1:2]),
as.data.frame(tidy(wilcox.test(Q6_err2, Q6_err3))[,1:2]),
#All
as.data.frame(tidy(wilcox.test(combined_err1, combined_err2))[,1:2]),
as.data.frame(tidy(wilcox.test(combined_err1, combined_err3))[,1:2]),
as.data.frame(tidy(wilcox.test(combined_err2, combined_err3))[,1:2])
) %>%
  mutate(Significance = ifelse(p.value <= 0.05, "*", ""))

ttest_table <- cbind(question, comparison, wilcox)

ttest_table %>%
  kable(caption = "Wilcoxon two-sided rank sum test with continuity correction",
        col.names = c("Question", "Groups Compared", "Statistic", "P-Value", "Significant"),
        format = "html")  
```

More graphical evidence is provided below.

```{r more_graphs}
#Q2
ggplot(experimental_data, aes(Q2err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Error distribution for Question 2",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")

#Q3
ggplot(experimental_data, aes(Q3err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Error distribution for Question 3",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")

#Q4
ggplot(experimental_data, aes(Q4err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Error distribution for Question 4",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")

#Q5
ggplot(experimental_data, aes(Q5err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Error distribution for Question 5",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")

#Q6
ggplot(experimental_data, aes(Q6err)) + 
  geom_density(aes(color = treatment)) + 
  labs(title = "Error distribution for Question 6",
       subtitle = "All responses",
       x = "Error",
       y = "Density",
       color = "Treatment\nGroup",
       caption = "N = 151")
```

## Limitations and counter-arguments

As previously discussed, the experimental design has some important flaws. First, the wording and presentation were evidently unclear to many of the participants. Almost half of all participants provided responses that were unsuable in the end. Since I did not limit the participant pool by any selection criteria, it is likely that poor English reading comprehension was an issue. This could have been addressed by limiting the pool of eligible workers to those located in the US and with a certain level of education. However, the wording of the questions itself was less than clear. A few native English-speakers contacted me by e-mail, stating that they had minsunderstood the questions. In addition, to make the expectations of the experiment abundantly clear, I could have included a training HIT or some examples at the beginning of the HIT.

Secondly, the arguments involved in hypotheses 2 and 3 are not mutually exclusive. Thus, the methodology used cannot fully address all three hypotheses. For example, selectively colored bar charts where only the relevant parts were colored and the rest uncolored, by themselves, do not fully control for the attention-grabbing effect of color that hypothesis 2 is concerned with. To extend the analysis, it may be helpful to use charts using selective coloring for relevant parts in addition to using color elsewhere in the chart, and also to experiment with different colors and forms of shading than what was used in this study.

### Citations

Cleveland, William S., and Robert McGill. 1984. "Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods." *Journal of the American Statistical Association* 79 (387): 531-54.

Heer, Jeffrey, and Michael Bostock. 2010. "Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design." *CHI*, 203-12.