---
title: "Text visualization"
author: |
  | MACS 40700
  | University of Chicago
date: "May 22, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE)
```

```{r packages, cache = FALSE, message = FALSE, include = FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
library(broom)
library(stringr)
library(modelr)
library(forcats)
library(tidytext)
library(twitteR)
library(wordcloud)
library(scales)

options(digits = 3)
set.seed(1234)

base_size <- 22
theme_set(theme_minimal(base_size = base_size))
```

## Basic workflow for text analysis

* Obtain your text sources
* Extract documents and move into a corpus
* Transformation
* Extract features
* Perform analysis

## Obtain your text sources

* Web sites
    * Twitter
* Databases
* PDF documents
* Digital scans of printed materials

## Extract documents and move into a corpus

* Corpus
* Character string

## Transformation

* Tagging segments of speech for part-of-speech (nouns, verbs, adjectives, etc.) or entity recognition (person, place, company, etc.)
* Standard text processing
    * Converting to lower case
    * Removing punctuation
    * Removing numbers
    * Removing stopwords
    * Removing domain-specific stopwords
    * Stemming

## Extract features

* Quantify your measures
* Bag-of-words model
* Term-document matrix
    * Each row is a document
    * Each column is a term
    * Each cell represents the frequency of the term appearing in the document
* Missing context

## Perform analysis

* Word frequency
* Collocation
* Dictionary tagging
* Document classification
* Corpora comparison
* Topic modeling

## Wordclouds

* Compile error - [see notes](day17-notes.html#wordclouds)

## N-gram viewers

* N-gram
* [Google Books Ngram Viewer](https://books.google.com/ngrams)
    * [Calendar of Meaningful Dates](https://www.xkcd.com/1140/)
* [How The Internet* Talks](https://projects.fivethirtyeight.com/reddit-ngram/?keyword=triggered.safe_space.sjw.snowflake&start=20071015&end=20161231&smoothing=10)

## Geospatial visualization

* [Which Curse Words Are Popular In Your State? Find Out From These Maps.](http://www.huffingtonpost.com/entry/which-curse-words-are-popular-in-your-state_us_55a80662e4b04740a3df54b8)
* [Hate Map](http://users.humboldt.edu/mstephens/hate/hate_map.html)
* [Soda vs. Pop with Twitter](http://blog.echen.me/2012/07/06/soda-vs-pop-with-twitter/)

## Network analysis with text

* Use text features to identify edges between nodes in a network
* [How every #GameOfThrones episode has been discussed on Twitter](https://interactive.twitter.com/game-of-thrones/#?episode=1)

## Sentiment analysis

> I am happy

## Dictionaries

```{r}
get_sentiments("bing")
```

## Dictionaries

```{r}
get_sentiments("afinn")
```

## Dictionaries

```{r}
get_sentiments("nrc")
```

## Dictionaries

```{r}
get_sentiments("nrc") %>%
  count(sentiment)
```

## Measuring overall sentiment {.scrollable}

```{r}
library(janeaustenr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text,
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
        geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
        facet_wrap(~book, ncol = 2, scales = "free_x")
```

## \@realDonaldTrump

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Every non-hyperbolic tweet is from iPhone (his staff). <br><br>Every hyperbolic tweet is from Android (from him). <a href="https://t.co/GWr6D8h5ed">pic.twitter.com/GWr6D8h5ed</a></p>&mdash; Todd Vaziri (\@tvaziri) <a href="https://twitter.com/tvaziri/status/762005541388378112">August 6, 2016</a></blockquote>
<script async src="http://platform.twitter.com/widgets.js" charset="utf-8"></script>

## Obtaining documents

```{r}
library(twitteR)
```

```{r}
# You'd need to set global options with an authenticated app
setup_twitter_oauth(getOption("twitter_api_key"),
                    getOption("twitter_api_token"))
```

```{r, eval = FALSE}
# We can request only 3200 tweets at a time; it will return fewer
# depending on the API
trump_tweets <- userTimeline("realDonaldTrump", n = 3200)
trump_tweets_df <- trump_tweets %>%
  map_df(as.data.frame) %>%
  tbl_df()
```

```{r trump_tweets_df}
# if you want to follow along without setting up Twitter authentication,
# just use this dataset:
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
str(trump_tweets_df)
```

## Clean up the data

```{r tweets, dependson = "trump_tweets_df"}
tweets <- trump_tweets_df %>%
  select(id, statusSource, text, created) %>%
  extract(statusSource, "source", "Twitter for (.*?)<") %>%
  filter(source %in% c("iPhone", "Android"))

tweets %>%
  head() %>%
  knitr::kable()
```

## Trump tweets

```{r}
library(tidytext)

# function to neatly print the first 10 rows using kable
print_neat <- function(df){
  df %>%
    head() %>%
    knitr::kable()
}

# tweets data frame
tweets %>%
  print_neat()
```

## Remove manual retweets

```{r}
# remove manual retweets
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  print_neat()
```

## Tokenize

```{r}
# custom regular expression to tokenize tweets
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

# unnest into tokens - tidytext format
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  print_neat()
```

## Remove stop words

```{r}
# remove stop words
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>%
  print_neat()
```

```{r, include = FALSE}
# store for future use
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
```

## Frequency of tokens

```{r tweet_words_plot, dependson = "tweet_words", fig.height = 6, fig.width = 8, echo = FALSE}
tweet_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  labs(title = "Frequency of tokens",
       subtitle = "@realDonaldTrump",
       x = "Word",
       y = "Occurrences") +
  coord_flip()
```

## Assessing word importance

* Term frequency (tf)
* Inverse document frequency (idf)
* Term frequency-inverse document frequency (tf-idf)

    $$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$

## Calculate tf-idf

```{r}
tweet_words_count <- tweet_words %>%
  count(source, word, sort = TRUE) %>%
  ungroup()
tweet_words_count
```

## Calculate tf-idf

```{r}
total_words <- tweet_words_count %>%
  group_by(source) %>%
  summarize(total = sum(n))
total_words
```

## Calculate tf-idf

```{r}
tweet_words_count <- left_join(tweet_words_count, total_words)
tweet_words_count
```

## Calculate tf-idf

```{r}
tweet_words_count <- tweet_words_count %>%
  bind_tf_idf(word, source, n)
tweet_words_count
```

## Which terms have a high tf-idf?

```{r}
tweet_words_count %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

## Which terms have a high tf-idf?

```{r}
tweet_important <- tweet_words_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

tweet_important %>%
  group_by(source) %>%
  slice(1:15) %>%
  ggplot(aes(word, tf_idf, fill = source)) +
  geom_bar(alpha = 0.8, stat = "identity") +
  labs(title = "Highest tf-idf words",
       subtitle = "Top 15 for Android and iPhone",
       x = NULL, y = "tf-idf") +
  coord_flip()
```

## Sentiment analysis

```{r nrc}
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)
nrc
```

## Sentiment analysis

```{r by_source_sentiment}
sources <- tweet_words %>%
  group_by(source) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(id, source, total_words)
sources
```

## Sentiment analysis

```{r}
by_source_sentiment <- tweet_words %>%
  inner_join(nrc, by = "word") %>%
  count(sentiment, id) %>%
  ungroup() %>%
  complete(sentiment, id, fill = list(n = 0)) %>%
  inner_join(sources) %>%
  group_by(source, sentiment, total_words) %>%
  summarize(words = sum(n)) %>%
  ungroup()

head(by_source_sentiment)
```

## Is this significant?

```{r}
# function to calculate the poisson.test for a given sentiment
poisson_test <- function(df){
  poisson.test(df$words, df$total_words)
}

# use the nest() and map() functions to apply poisson_test to each
# sentiment and extract results using broom::tidy()
sentiment_differences <- by_source_sentiment %>%
  group_by(sentiment) %>%
  nest() %>%
  mutate(poisson = map(data, poisson_test),
         poisson_tidy = map(poisson, tidy)) %>%
  unnest(poisson_tidy, .drop = TRUE)
sentiment_differences
```

## Is this significant?

```{r}
sentiment_differences %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, estimate)) %>%
  mutate_each(funs(. - 1), estimate, conf.low, conf.high) %>%
  ggplot(aes(estimate, sentiment)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "% increase in Android relative to iPhone",
       y = "Sentiment")
```

## Most important words

```{r, fig.height = 10}
tweet_important %>%
  inner_join(nrc, by = "word") %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  mutate(sentiment = reorder(sentiment, -tf_idf),
         word = reorder(word, -tf_idf)) %>%
  group_by(sentiment) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = source)) +
  facet_wrap(~ sentiment, scales = "free", nrow = 4) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "",
       y = "tf-idf") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
```

## Bag-of-words

```
 a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1
```

1. Sparsity
1. Stop words
1. Correlation between words

## Latent semantic analysis

* Identify words closely related to one another
* Makes searching by keyword easier
* Uses PCA or similar techinques

## `NYTimes`

```{r nytimes}
# get NYTimes data
load("data/pca-examples.Rdata")
```

```{r nytimes-words}
colnames(nyt.frame)[sample(ncol(nyt.frame),30)]
```

## `NYTimes`

```{r nytimes-pca}
# Omit the first column of class labels
nyt.pca <- prcomp(nyt.frame[,-1])

# Extract the actual component directions/weights for ease of reference
nyt.latent.sem <- nyt.pca$rotation

# convert to data frame
nyt.latent.sem <- nyt.latent.sem %>%
  as_tibble %>%
  mutate(word = names(nyt.latent.sem[,1])) %>%
  select(word, everything())
```

```{r nytimes-PC1}
nyt.latent.sem %>%
  select(word, PC1) %>%
  arrange(PC1) %>%
  slice(c(1:10, (n() - 10):n())) %>%
  mutate(pos = ifelse(PC1 > 0, TRUE, FALSE),
         word = fct_reorder(word, PC1)) %>%
  ggplot(aes(word, PC1, fill = pos)) +
  geom_col() +
  labs(title = "LSA analysis of NYTimes articles",
       x = NULL,
       y = "PC1 scores") +
  coord_flip() +
  theme(legend.position = "none")
```

## `NYTimes`

```{r nytimes-PC2}
nyt.latent.sem %>%
  select(word, PC2) %>%
  arrange(PC2) %>%
  slice(c(1:10, (n() - 10):n())) %>%
  mutate(pos = ifelse(PC2 > 0, TRUE, FALSE),
         word = fct_reorder(word, PC2)) %>%
  ggplot(aes(word, PC2, fill = pos)) +
  geom_col() +
  labs(title = "LSA analysis of NYTimes articles",
       x = NULL,
       y = "PC2 scores") +
  coord_flip() +
  theme(legend.position = "none")
```

## `NYTimes`

```{r nytimes-biplot}
biplot(nyt.pca, scale = 0, cex = .6)
```

## `NYTimes`

```{r nytimes-plot-dim}
cbind(type = nyt.frame$class.labels, as_tibble(nyt.pca$x[,1:2])) %>%
  mutate(type = factor(type, levels = c("art", "music"),
                       labels = c("A", "M"))) %>%
  ggplot(aes(PC1, PC2, label = type, color = type)) +
  geom_text() +
  labs(title = "")
  theme(legend.position = "none")
```


